{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0dea974-236c-4f81-9b2e-9c041f34fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import pyro.distributions as dist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "pyro.set_rng_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d9796-8d37-44e3-8b53-1045b6670eca",
   "metadata": {},
   "source": [
    "# M4: Multistage (k=8, inner=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3513c05e-d59d-403d-b137-511cbbbfa4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   B01001_001E  Household Income Distribution  Median Household Income  \\\n",
      "0       1675.0                          860.0                  75444.0   \n",
      "1       2221.0                         1070.0                 140313.0   \n",
      "2       1904.0                         1098.0                  83750.0   \n",
      "3       1147.0                          517.0                  62054.0   \n",
      "4       4946.0                         3231.0                  29737.0   \n",
      "\n",
      "   B19001_002E  B19001_003E  B19001_004E  B19001_005E  B19001_006E  \\\n",
      "0         23.0         98.0         21.0         10.0         52.0   \n",
      "1         45.0         24.0         16.0         17.0          0.0   \n",
      "2         72.0         76.0         50.0         72.0          0.0   \n",
      "3          0.0          0.0          0.0         60.0         61.0   \n",
      "4        373.0        471.0        391.0        176.0        217.0   \n",
      "\n",
      "   B19001_007E  B19001_008E  ...  Longitude  storm_county    ppt_mean  \\\n",
      "0         10.0          9.0  ...   -80.3117  201007_12086  117.744385   \n",
      "1         32.0         16.0  ...   -80.3117  201007_12086  117.744385   \n",
      "2         60.0         38.0  ...   -80.3117  201007_12086  117.744385   \n",
      "3         48.0          0.0  ...   -80.3117  201007_12086  117.744385   \n",
      "4        250.0        176.0  ...   -80.3117  201007_12086  117.744385   \n",
      "\n",
      "   tmean_mean  Year  cluster  pct_unemp  pct_crowd    log_pop  log_medinc  \n",
      "0    23.45584  2010        2   0.042706   0.043913  14.709709   11.231159  \n",
      "1    23.45584  2010        2   0.042706   0.043913  14.709709   11.851638  \n",
      "2    23.45584  2010        2   0.042706   0.043913  14.709709   11.335603  \n",
      "3    23.45584  2010        2   0.042706   0.043913  14.709709   11.035776  \n",
      "4    23.45584  2010        2   0.042706   0.043913  14.709709   10.300181  \n",
      "\n",
      "[5 rows x 60 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_24104/4225642204.py:7: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "file_path = \"cleaned_hurricane_damage_data.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd1e72b-75f6-4f31-ae6a-dddd5660c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 746 | Test size: 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_24104/3641305764.py:113: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full = pd.read_csv(\"cleaned_hurricane_damage_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ─────────────────────────── Column Mapping ───────────────────────────\n",
    "census_svi_column_mapping = {\n",
    "    'B01001_001E': 'Total Population',\n",
    "    'B19001_001E': 'Household Income Distribution',\n",
    "    'B19013_001E': 'Median Household Income',\n",
    "    'B19001_002E': 'Income Bracket 1',\n",
    "    'B19001_003E': 'Income Bracket 2',\n",
    "    'B19001_004E': 'Income Bracket 3',\n",
    "    'B19001_005E': 'Income Bracket 4',\n",
    "    'B19001_006E': 'Income Bracket 5',\n",
    "    'B19001_007E': 'Income Bracket 6',\n",
    "    'B19001_008E': 'Income Bracket 7',\n",
    "    'B19001_009E': 'Income Bracket 8',\n",
    "    'B19001_010E': 'Income Bracket 9',\n",
    "    'B19001_011E': 'Income Bracket 10',\n",
    "    'B19001_012E': 'Income Bracket 11',\n",
    "    'B19001_013E': 'Income Bracket 12',\n",
    "    'B19001_014E': 'Income Bracket 13',\n",
    "    'B19001_015E': 'Income Bracket 14',\n",
    "    'B19001_016E': 'Income Bracket 15',\n",
    "    'B19001_017E': 'Income Bracket 16',\n",
    "    'state': 'State',\n",
    "    'county': 'County',\n",
    "    'tract': 'Census Tract',\n",
    "    'year': 'Year',\n",
    "    'STATE_COUNTY_FIPS': 'State-County FIPS Code',\n",
    "    'GEOID': 'Geographic Identifier',\n",
    "    'FIPS': 'FIPS Code',\n",
    "    'RPL_THEMES': 'SVI Themes',\n",
    "    'CZ_FIPS': 'Combined Statistical Area FIPS',\n",
    "    'ST': 'State Abbreviation',\n",
    "    'LOCATION': 'Location',\n",
    "    'E_TOTPOP': 'Estimated Total Population',\n",
    "    'M_TOTPOP': 'Margin of Error Total Population',\n",
    "    'E_HU': 'Estimated Housing Units',\n",
    "    'M_HU': 'Margin of Error Housing Units',\n",
    "    'E_UNEMP': 'Estimated Unemployed',\n",
    "    'M_UNEMP': 'Margin of Error Unemployed',\n",
    "    'E_LIMENG': 'Estimated Limited English Proficiency',\n",
    "    'M_LIMENG': 'Margin of Error Limited English Proficiency',\n",
    "    'E_MUNIT': 'Estimated Multi-Unit Housing',\n",
    "    'M_MUNIT': 'Margin of Error Multi-Unit Housing',\n",
    "    'E_MOBILE': 'Estimated Mobile Homes',\n",
    "    'M_MOBILE': 'Margin of Error Mobile Homes',\n",
    "    'E_CROWD': 'Estimated Crowded Housing',\n",
    "    'M_CROWD': 'Margin of Error Crowded Housing',\n",
    "    'E_NOVEH': 'Estimated No Vehicle',\n",
    "    'M_NOVEH': 'Margin of Error No Vehicle',\n",
    "    'DAMAGE_PROPERTY': 'Damage to Property',\n",
    "    'DAMAGE_CROPS': 'Damage to Crops',\n",
    "    'BEGIN_YEARMONTH': 'Begin Year-Month',\n",
    "    'LAT': 'Latitude',\n",
    "    'LON': 'Longitude',\n",
    "    'storm_county': 'Storm County'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────── Feature Engineering ───────────────────────────\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    eps = 1e-6\n",
    "\n",
    "    df['pct_unemp']  = df['Estimated Unemployed'] / (df['Total Population'] + eps)\n",
    "    df['pct_limm']   = df['Estimated Limited English Proficiency'] / (df['Total Population'] + eps)\n",
    "    df['pct_noveh']  = df['Estimated No Vehicle'] / (df['Total Population'] + eps)\n",
    "    df['pct_mobile'] = df['Estimated Mobile Homes'] / (df['Estimated Housing Units'] + eps)\n",
    "    df['pct_crowd']  = df['Estimated Crowded Housing'] / (df['Estimated Housing Units'] + eps)\n",
    "\n",
    "    lows  = ['Income Bracket 1', 'Income Bracket 2', 'Income Bracket 3']\n",
    "    highs = ['Income Bracket 14', 'Income Bracket 15', 'Income Bracket 16']\n",
    "    df['low_inc_pct']  = df[lows].sum(axis=1) / (df['Household Income Distribution'] + eps)\n",
    "    df['high_inc_pct'] = df[highs].sum(axis=1) / (df['Household Income Distribution'] + eps)\n",
    "    df['inc_ineq']     = df['high_inc_pct'] / (df['low_inc_pct'] + eps)\n",
    "\n",
    "    df['log_pop']    = np.log1p(df['Total Population'])\n",
    "    df['log_medinc'] = np.log1p(df['Median Household Income'])\n",
    "    df['pop_poverty'] = df['Total Population'] * df['low_inc_pct']\n",
    "\n",
    "    df['Begin Year-Month'] = pd.to_datetime(df['Begin Year-Month'], format='%Y%m')\n",
    "    df['month'] = df['Begin Year-Month'].dt.month\n",
    "\n",
    "    return df\n",
    "\n",
    "features = [\n",
    "    'log_pop', 'log_medinc',\n",
    "    'pct_unemp', 'pct_limm', 'pct_noveh',\n",
    "    'low_inc_pct', 'pop_poverty', 'high_inc_pct',\n",
    "    'ppt_mean', 'tmean_mean'\n",
    "]\n",
    "\n",
    "# ─────────────────────────── Main Pipeline ───────────────────────────\n",
    "# Load and rename\n",
    "full = pd.read_csv(\"cleaned_hurricane_damage_data.csv\")\n",
    "full = full.rename(columns=census_svi_column_mapping)\n",
    "full = full.loc[:, ~full.columns.duplicated()]\n",
    "\n",
    "# Remove rows with no damage\n",
    "full = full[full['Damage to Property'] != 0].dropna(subset=['Damage to Property'])\n",
    "\n",
    "# Add derived features\n",
    "full = add_features(full)\n",
    "\n",
    "# Group target (sum) and features (mean)\n",
    "y_group = full.groupby(['Year', 'State-County FIPS Code'])[['Damage to Property']].sum().reset_index()\n",
    "x_group = full.groupby(['Year', 'State-County FIPS Code'])[features + ['Latitude', 'Longitude']].mean().reset_index()\n",
    "\n",
    "# Merge target and predictors\n",
    "full_data = y_group.merge(x_group, on=['Year', 'State-County FIPS Code'])\n",
    "\n",
    "# ─────────────────────────── Train-Test Split ───────────────────────────\n",
    "train_full = full_data[full_data['Year'] < 2020].reset_index(drop=True)\n",
    "test_full  = full_data[full_data['Year'] == 2020].reset_index(drop=True)\n",
    "\n",
    "# ─────────────────────────── Spatial Clustering ───────────────────────────\n",
    "coords = train_full[['Latitude', 'Longitude']].values\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(coords)\n",
    "train_full['spatial_cluster'] = kmeans.labels_\n",
    "\n",
    "print(f\"Train size: {train_full.shape[0]} | Test size: {test_full.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc801b2a-56c5-4658-9f61-ba4ce147be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_Xy(df):\n",
    "    df = df.dropna(subset=features + ['Damage to Property'])\n",
    "    X = df[features].values\n",
    "    y = np.log1p(df['Damage to Property'].values)\n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de87b3-e9e6-488d-988a-ce082f9268d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb5ca385-bfde-43dd-90c4-6544183f8caa",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7af2f905-586d-44e5-9603-3000d10bdbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OUTER Fold 1/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_24104/368445547.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outer_train['inner_cluster'] = inner_kmeans.labels_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train Loss=2732071.8, Val Loss=6041.4\n",
      "Epoch 1000: Train Loss=26464.1, Val Loss=10935.1\n",
      "Epoch 1500: Train Loss=65671.5, Val Loss=11982.8\n",
      "Epoch 2000: Train Loss=66312364.8, Val Loss=830.6\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "Epoch 500: Train Loss=201232.5, Val Loss=16295.3\n",
      "Epoch 1000: Train Loss=15496833.2, Val Loss=7889.7\n",
      "Epoch 1500: Train Loss=244506.8, Val Loss=9297.7\n",
      "Epoch 2000: Train Loss=496208.2, Val Loss=2395.6\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "Epoch 500: Train Loss=1426559.1, Val Loss=123855.5\n",
      "Epoch 1000: Train Loss=148656.4, Val Loss=7399.2\n",
      "Epoch 1500: Train Loss=299136.2, Val Loss=8921.9\n",
      "Epoch 2000: Train Loss=9107.4, Val Loss=2129.7\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "Epoch 500: Train Loss=8022.0, Val Loss=24801.2\n",
      "Epoch 1000: Train Loss=4933.7, Val Loss=32296.9\n",
      "Epoch 1500: Train Loss=1118756.9, Val Loss=4147.2\n",
      "Epoch 2000: Train Loss=8752.7, Val Loss=36584.3\n",
      "\n",
      "=== OUTER Fold 2/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_24104/368445547.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outer_train['inner_cluster'] = inner_kmeans.labels_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train Loss=29116.6, Val Loss=7769.1\n",
      "Epoch 1000: Train Loss=330229784.7, Val Loss=11751.6\n",
      "Epoch 1500: Train Loss=2601166.1, Val Loss=113444.7\n",
      "Epoch 2000: Train Loss=1207359.9, Val Loss=7101.5\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "Epoch 500: Train Loss=517622.3, Val Loss=770659.5\n",
      "Epoch 1000: Train Loss=3589.0, Val Loss=3905.1\n",
      "Epoch 1500: Train Loss=13670.0, Val Loss=1548.9\n",
      "Epoch 2000: Train Loss=7802.7, Val Loss=1730.4\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "Epoch 500: Train Loss=34572.3, Val Loss=3467.5\n",
      "Epoch 1000: Train Loss=44966.2, Val Loss=7127.1\n",
      "Epoch 1500: Train Loss=4716.0, Val Loss=5087.4\n",
      "Epoch 2000: Train Loss=98197166.7, Val Loss=65580.1\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "Epoch 500: Train Loss=120207.2, Val Loss=3463835.5\n",
      "Epoch 1000: Train Loss=3312.5, Val Loss=90917.0\n",
      "Epoch 1500: Train Loss=3815.7, Val Loss=378.5\n",
      "Epoch 2000: Train Loss=24824.8, Val Loss=1041.0\n",
      "\n",
      "=== OUTER Fold 3/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_24104/368445547.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outer_train['inner_cluster'] = inner_kmeans.labels_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train Loss=4408.6, Val Loss=23639.6\n",
      "Epoch 1000: Train Loss=235177.4, Val Loss=21984.9\n",
      "Epoch 1500: Train Loss=2919.4, Val Loss=376517.6\n",
      "Epoch 2000: Train Loss=6182.0, Val Loss=8279.9\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "Epoch 500: Train Loss=4046.1, Val Loss=10192.9\n",
      "Epoch 1000: Train Loss=37753.7, Val Loss=8767.0\n",
      "Epoch 1500: Train Loss=5601.7, Val Loss=6976.0\n",
      "Epoch 2000: Train Loss=2382.9, Val Loss=2591.8\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "Epoch 500: Train Loss=1319499.3, Val Loss=10349.4\n",
      "Epoch 1000: Train Loss=11074.2, Val Loss=2168313.1\n",
      "Epoch 1500: Train Loss=2832980.5, Val Loss=18890.0\n",
      "Epoch 2000: Train Loss=11461.5, Val Loss=6508.1\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "Epoch 500: Train Loss=116133.9, Val Loss=342598.0\n",
      "Epoch 1000: Train Loss=3302.5, Val Loss=537.6\n",
      "Epoch 1500: Train Loss=31632.7, Val Loss=481.9\n",
      "Epoch 2000: Train Loss=4209.3, Val Loss=399.4\n",
      "\n",
      "=== OUTER Fold 4/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_24104/368445547.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outer_train['inner_cluster'] = inner_kmeans.labels_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train Loss=4835.0, Val Loss=35331928.8\n",
      "Epoch 1000: Train Loss=153264.4, Val Loss=52272.1\n",
      "Epoch 1500: Train Loss=4728.9, Val Loss=692.1\n",
      "Epoch 2000: Train Loss=3602.8, Val Loss=673.8\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "Epoch 500: Train Loss=27886.9, Val Loss=68513.3\n",
      "Epoch 1000: Train Loss=8708.5, Val Loss=1908.2\n",
      "Epoch 1500: Train Loss=5874.7, Val Loss=5165.3\n",
      "Epoch 2000: Train Loss=8554.3, Val Loss=10929.5\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "Epoch 500: Train Loss=701488.9, Val Loss=4448.6\n",
      "Epoch 1000: Train Loss=21033.6, Val Loss=17406.5\n",
      "Epoch 1500: Train Loss=4548.9, Val Loss=6524.9\n",
      "Epoch 2000: Train Loss=15796754.4, Val Loss=728.6\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "Epoch 500: Train Loss=435688.0, Val Loss=254924.1\n",
      "Epoch 1000: Train Loss=12207.6, Val Loss=22310.8\n",
      "Epoch 1500: Train Loss=14735.5, Val Loss=3972206.1\n",
      "Epoch 2000: Train Loss=8022.7, Val Loss=107639.9\n",
      "\n",
      "=== OUTER Fold 5/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_24104/368445547.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outer_train['inner_cluster'] = inner_kmeans.labels_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train Loss=5356.0, Val Loss=4615.8\n",
      "Epoch 1000: Train Loss=11488.0, Val Loss=1908.1\n",
      "Epoch 1500: Train Loss=2887.9, Val Loss=1317.3\n",
      "Epoch 2000: Train Loss=11176.5, Val Loss=33393.9\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "Epoch 500: Train Loss=16527.1, Val Loss=403793.5\n",
      "Epoch 1000: Train Loss=27631.1, Val Loss=1244.1\n",
      "Epoch 1500: Train Loss=11718.7, Val Loss=1073.4\n",
      "Epoch 2000: Train Loss=9823.6, Val Loss=1652.0\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "Epoch 500: Train Loss=7661.6, Val Loss=3297.6\n",
      "Epoch 1000: Train Loss=6391315.3, Val Loss=4303.4\n",
      "Epoch 1500: Train Loss=6391.7, Val Loss=1306.1\n",
      "Epoch 2000: Train Loss=1957.6, Val Loss=1063.5\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "Epoch 500: Train Loss=68144.9, Val Loss=17441.7\n",
      "Epoch 1000: Train Loss=256443.1, Val Loss=3803.8\n",
      "Epoch 1500: Train Loss=4326733.7, Val Loss=2657.5\n",
      "Epoch 2000: Train Loss=7962.5, Val Loss=2033.4\n",
      "\n",
      "=== OUTER Fold 6/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_24104/368445547.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outer_train['inner_cluster'] = inner_kmeans.labels_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train Loss=9415.9, Val Loss=2042.3\n",
      "Epoch 1000: Train Loss=21420.9, Val Loss=80891.7\n",
      "Epoch 1500: Train Loss=13679.0, Val Loss=482.2\n",
      "Epoch 2000: Train Loss=15059.1, Val Loss=39387.8\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "Epoch 500: Train Loss=150263.9, Val Loss=45931.0\n",
      "Epoch 1000: Train Loss=2480.4, Val Loss=105244.7\n",
      "Epoch 1500: Train Loss=3487.8, Val Loss=4027.9\n",
      "Epoch 2000: Train Loss=5024.4, Val Loss=8985.0\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "Epoch 500: Train Loss=19259.7, Val Loss=78845.7\n",
      "Epoch 1000: Train Loss=860904.5, Val Loss=637462.5\n",
      "Epoch 1500: Train Loss=24078.6, Val Loss=2299.4\n",
      "Epoch 2000: Train Loss=4677.3, Val Loss=711.0\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "Epoch 500: Train Loss=387989.4, Val Loss=5771465715.0\n",
      "Epoch 1000: Train Loss=2043.0, Val Loss=284389.5\n",
      "Epoch 1500: Train Loss=153976.5, Val Loss=13924.0\n",
      "Epoch 2000: Train Loss=620830.2, Val Loss=1598.5\n",
      "\n",
      "=== OUTER Fold 7/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_24104/368445547.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outer_train['inner_cluster'] = inner_kmeans.labels_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train Loss=10455.3, Val Loss=6615.8\n",
      "Epoch 1000: Train Loss=3849.3, Val Loss=3130226.6\n",
      "Epoch 1500: Train Loss=9006.3, Val Loss=8936.5\n",
      "Epoch 2000: Train Loss=1090351.3, Val Loss=980811.5\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "Epoch 500: Train Loss=146330.4, Val Loss=3874.8\n",
      "Epoch 1000: Train Loss=14767.4, Val Loss=16953.4\n",
      "Epoch 1500: Train Loss=61729.8, Val Loss=534067.3\n",
      "Epoch 2000: Train Loss=104099.0, Val Loss=1421.8\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "Epoch 500: Train Loss=131796.9, Val Loss=10060.6\n",
      "Epoch 1000: Train Loss=10617.1, Val Loss=16488.1\n",
      "Epoch 1500: Train Loss=11882.2, Val Loss=86218.7\n",
      "Epoch 2000: Train Loss=52033.4, Val Loss=2380.2\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "Epoch 500: Train Loss=66773.7, Val Loss=2172.6\n",
      "Epoch 1000: Train Loss=321117.2, Val Loss=4054.4\n",
      "Epoch 1500: Train Loss=10655.2, Val Loss=5358.0\n",
      "Epoch 2000: Train Loss=25731.3, Val Loss=5488.7\n",
      "\n",
      "=== OUTER Fold 8/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_24104/368445547.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outer_train['inner_cluster'] = inner_kmeans.labels_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train Loss=11753.4, Val Loss=306365.5\n",
      "Epoch 1000: Train Loss=4872.4, Val Loss=1586.3\n",
      "Epoch 1500: Train Loss=3786.3, Val Loss=12725.1\n",
      "Epoch 2000: Train Loss=15091.6, Val Loss=38095.2\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "Epoch 500: Train Loss=1166972.4, Val Loss=16021.9\n",
      "Epoch 1000: Train Loss=265482.2, Val Loss=1953.6\n",
      "Epoch 1500: Train Loss=8764.5, Val Loss=649649.6\n",
      "Epoch 2000: Train Loss=7716.8, Val Loss=64533.6\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "Epoch 500: Train Loss=172581.6, Val Loss=6999.5\n",
      "Epoch 1000: Train Loss=58504.8, Val Loss=5047.9\n",
      "Epoch 1500: Train Loss=509330.1, Val Loss=4426.6\n",
      "Epoch 2000: Train Loss=257922.0, Val Loss=12345.2\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "Epoch 500: Train Loss=250162.3, Val Loss=2362.5\n",
      "Epoch 1000: Train Loss=34012.2, Val Loss=3301.2\n",
      "Epoch 1500: Train Loss=74873.8, Val Loss=5949.5\n",
      "Epoch 2000: Train Loss=39605.0, Val Loss=2275.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import pyro.distributions as dist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "def safe_metrics(y_true, y_pred):\n",
    "    mask = (\n",
    "        (~np.isnan(y_true)) & (~np.isnan(y_pred)) &\n",
    "        (~np.isinf(y_true)) & (~np.isinf(y_pred))\n",
    "    )\n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "    return mae, rmse\n",
    "\n",
    "def nested_spatial_cv_evaluation(train_data, features, outer_k=10, inner_k=4, n_epochs=2000):\n",
    "    outer_coords = train_data[['Latitude', 'Longitude']].values\n",
    "    outer_kmeans = KMeans(n_clusters=outer_k, random_state=0).fit(outer_coords)\n",
    "    train_data['outer_cluster'] = outer_kmeans.labels_\n",
    "\n",
    "    outer_results = []\n",
    "    all_error_dfs = []\n",
    "\n",
    "    for outer_fold in range(outer_k):\n",
    "        print(f\"\\n=== OUTER Fold {outer_fold+1}/{outer_k} ===\")\n",
    "        outer_val = train_data[train_data['outer_cluster'] == outer_fold]\n",
    "        outer_train = train_data[train_data['outer_cluster'] != outer_fold]\n",
    "\n",
    "        inner_coords = outer_train[['Latitude', 'Longitude']].values\n",
    "        inner_kmeans = KMeans(n_clusters=inner_k, random_state=0).fit(inner_coords)\n",
    "        outer_train['inner_cluster'] = inner_kmeans.labels_\n",
    "\n",
    "        best_model_params = None\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for inner_fold in range(inner_k):\n",
    "            print(f\"\\n--- INNER Fold {inner_fold+1}/{inner_k} ---\")\n",
    "            inner_val = outer_train[outer_train['inner_cluster'] == inner_fold]\n",
    "            inner_train = outer_train[outer_train['inner_cluster'] != inner_fold]\n",
    "\n",
    "            X_train = inner_train[features].values\n",
    "            y_train = np.log1p(inner_train['Damage to Property'].values)\n",
    "            X_val = inner_val[features].values\n",
    "            y_val = np.log1p(inner_val['Damage to Property'].values)\n",
    "\n",
    "            scaler = StandardScaler().fit(X_train)\n",
    "            X_train_t = torch.tensor(scaler.transform(X_train), dtype=torch.float32)\n",
    "            X_val_t = torch.tensor(scaler.transform(X_val), dtype=torch.float32)\n",
    "            y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "            y_val_t = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "            def model(X, y=None):\n",
    "                n_features = X.shape[1]\n",
    "                intercept = pyro.sample(\"intercept\", dist.Normal(0., 10.))\n",
    "                weights = pyro.sample(\"weights\", dist.Normal(torch.zeros(n_features), 10 * torch.ones(n_features)).to_event(1))\n",
    "                sigma = pyro.sample(\"sigma\", dist.HalfNormal(10.))\n",
    "                mu = intercept + (X * weights).sum(-1)\n",
    "                with pyro.plate(\"data\", X.shape[0]):\n",
    "                    pyro.sample(\"obs\", dist.Normal(mu, sigma), obs=y)\n",
    "\n",
    "            def guide(X, y=None):\n",
    "                n_features = X.shape[1]\n",
    "                pyro.sample(\"intercept\", dist.Normal(pyro.param(\"intercept_loc\", torch.tensor(0.0)),\n",
    "                                                     pyro.param(\"intercept_scale\", torch.tensor(1.0), constraint=dist.constraints.positive)))\n",
    "                pyro.sample(\"weights\", dist.Normal(pyro.param(\"weights_loc\", torch.zeros(n_features)),\n",
    "                                                   pyro.param(\"weights_scale\", torch.ones(n_features), constraint=dist.constraints.positive)).to_event(1))\n",
    "                pyro.sample(\"sigma\", dist.HalfNormal(pyro.param(\"sigma_loc\", torch.tensor(1.0), constraint=dist.constraints.positive)))\n",
    "\n",
    "            pyro.clear_param_store()\n",
    "            svi = SVI(model, guide, Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "\n",
    "            for ep in range(1, n_epochs + 1):\n",
    "                train_loss = svi.step(X_train_t, y_train_t)\n",
    "                val_loss = svi.evaluate_loss(X_val_t, y_val_t)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                if ep % 500 == 0 or ep == n_epochs:\n",
    "                    print(f\"Epoch {ep}: Train Loss={train_loss:.1f}, Val Loss={val_loss:.1f}\")\n",
    "\n",
    "            final_val_loss = svi.evaluate_loss(X_val_t, y_val_t)\n",
    "            if final_val_loss < best_val_loss:\n",
    "                best_val_loss = final_val_loss\n",
    "                best_model_params = {\n",
    "                    'scaler': scaler,\n",
    "                    'weights': pyro.param(\"weights_loc\").detach().numpy(),\n",
    "                    'intercept': pyro.param(\"intercept_loc\").item()\n",
    "                }\n",
    "                best_train_losses = train_losses\n",
    "                best_val_losses = val_losses\n",
    "\n",
    "\n",
    "        # Evaluate on outer_val using best model\n",
    "        X_outer = outer_val[features].values\n",
    "        y_outer_log = np.log1p(outer_val['Damage to Property'].values)\n",
    "        scaler = best_model_params['scaler']\n",
    "        X_outer_scaled = scaler.transform(X_outer)\n",
    "        y_pred_log = best_model_params['intercept'] + X_outer_scaled @ best_model_params['weights']\n",
    "        y_pred_orig = np.expm1(y_pred_log)\n",
    "        y_true_orig = np.expm1(y_outer_log)\n",
    "\n",
    "        mae_log, rmse_log = safe_metrics(y_outer_log, y_pred_log)\n",
    "        mae_orig, rmse_orig = safe_metrics(y_true_orig, y_pred_orig)\n",
    "\n",
    "        error_df = pd.DataFrame({\n",
    "            'true_log': y_outer_log,\n",
    "            'pred_log': y_pred_log,\n",
    "            'true_orig': y_true_orig,\n",
    "            'pred_orig': y_pred_orig,\n",
    "            'Latitude': outer_val['Latitude'].values,\n",
    "            'Longitude': outer_val['Longitude'].values,\n",
    "            'outer_fold': outer_fold\n",
    "\n",
    "        })\n",
    "\n",
    "        error_df['abs_error_log'] = np.abs(error_df['true_log'] - error_df['pred_log'])\n",
    "        error_df['rmse_error_log'] = (error_df['true_log'] - error_df['pred_log']) ** 2\n",
    "        error_df['abs_error_orig'] = np.abs(error_df['true_orig'] - error_df['pred_orig'])\n",
    "        error_df['rmse_error_orig'] = (error_df['true_orig'] - error_df['pred_orig']) ** 2\n",
    "\n",
    "        all_error_dfs.append(error_df)\n",
    "\n",
    "        outer_results.append({\n",
    "        'fold': outer_fold,\n",
    "        'mae_log': mae_log,\n",
    "        'rmse_log': rmse_log,\n",
    "        'mae_orig': mae_orig,\n",
    "        'rmse_orig': rmse_orig,\n",
    "        'samples': len(y_true_orig),\n",
    "        'train_losses': best_train_losses,\n",
    "        'val_losses': best_val_losses\n",
    "    })\n",
    "\n",
    "        # Plot and save map\n",
    "        def save_cartopy_scatter(df, value_col, title, filename, cmap, vmax):\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "            ax.set_extent([df['Longitude'].min() - 1, df['Longitude'].max() + 1,\n",
    "                           df['Latitude'].min() - 1, df['Latitude'].max() + 1], crs=ccrs.PlateCarree())\n",
    "            ax.add_feature(cfeature.STATES.with_scale('50m'), edgecolor='black', linewidth=0.6)\n",
    "            sc = ax.scatter(df['Longitude'], df['Latitude'],\n",
    "                            c=df[value_col], cmap=cmap, s=30, alpha=0.85,\n",
    "                            transform=ccrs.PlateCarree(), vmin=0, vmax=vmax)\n",
    "            cbar = plt.colorbar(sc, ax=ax, orientation='vertical', shrink=0.8, pad=0.02)\n",
    "            cbar.set_label(title, fontsize=12)\n",
    "            cbar.ax.tick_params(labelsize=10)\n",
    "            ax.set_title(title, fontsize=14)\n",
    "            ax.set_xlabel(\"Longitude\")\n",
    "            ax.set_ylabel(\"Latitude\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "        vmax_log = max(error_df['abs_error_log'].max(), np.sqrt(error_df['rmse_error_log']).max())\n",
    "        vmax_orig = max(error_df['abs_error_orig'].max(), np.sqrt(error_df['rmse_error_orig']).max())\n",
    "\n",
    "        save_cartopy_scatter(error_df, 'abs_error_log', f'Fold {outer_fold} - Log MAE', f'M4_fold{outer_fold}_log_mae.png', 'viridis', vmax_log)\n",
    "        save_cartopy_scatter(error_df, 'rmse_error_log', f'Fold {outer_fold} - Log RMSE', f'M4_fold{outer_fold}_log_rmse.png', 'viridis', vmax_log)\n",
    "        save_cartopy_scatter(error_df, 'abs_error_orig', f'Fold {outer_fold} - MAE', f'M4_fold{outer_fold}_mae.png', 'plasma', vmax_orig)\n",
    "        save_cartopy_scatter(error_df, 'rmse_error_orig', f'Fold {outer_fold} - RMSE', f'M4_fold{outer_fold}_rmse.png', 'plasma', vmax_orig)\n",
    "\n",
    "    results_df = pd.DataFrame(outer_results)\n",
    "    full_error_df = pd.concat(all_error_dfs, ignore_index=True)\n",
    "    return results_df, full_error_df\n",
    "\n",
    "\n",
    "results_df_M4, full_error_df_M4 = nested_spatial_cv_evaluation(\n",
    "    train_data=train_full,\n",
    "    features=features,\n",
    "    outer_k=8,\n",
    "    inner_k=4,\n",
    "    n_epochs=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d47a968-30d2-4c1c-8e22-8fc9d344bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the csv directory if it doesn't exist\n",
    "os.makedirs('csv', exist_ok=True)\n",
    "\n",
    "# Save the DataFrames\n",
    "results_df_M4.to_csv('csv/results_df_M4.csv', index=True)\n",
    "full_error_df_M4.to_csv('csv/full_error_df_M4.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ee2805f-adb7-429a-aa3c-07bdd0a26212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined error map: M4_combined_log_mae.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "def plot_combined_log_mae(error_df, output_file=\"M4_combined_log_mae.png\"):\n",
    "    vmax = error_df['abs_error_log'].max()\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.set_extent([error_df['Longitude'].min() - 1, error_df['Longitude'].max() + 1,\n",
    "                   error_df['Latitude'].min() - 1, error_df['Latitude'].max() + 1])\n",
    "    ax.add_feature(cfeature.STATES.with_scale('50m'), edgecolor='black', linewidth=0.6)\n",
    "\n",
    "    sc = ax.scatter(error_df['Longitude'], error_df['Latitude'],\n",
    "                    c=error_df['abs_error_log'], cmap='viridis', s=30, alpha=0.8,\n",
    "                    transform=ccrs.PlateCarree(), vmin=0, vmax=vmax)\n",
    "\n",
    "    cbar = plt.colorbar(sc, ax=ax, orientation='vertical', shrink=0.8, pad=0.02)\n",
    "    cbar.set_label(\"Log MAE\", fontsize=12)\n",
    "    ax.set_title(\"Log MAE Error (All Outer Folds Combined)\", fontsize=14)\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved combined error map: {output_file}\")\n",
    "\n",
    "plot_combined_log_mae(full_error_df_M4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "378173a1-d85e-4e19-9842-7b8a69868382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out the following to look at the loss curves for outer clusters\n",
    "# def plot_loss_curves(outer_results, output_prefix=\"fold\"):\n",
    "#     for result in outer_results:\n",
    "#         train_losses = result['train_losses']\n",
    "#         val_losses = result['val_losses']\n",
    "#         fold = result['fold']\n",
    "\n",
    "#         plt.figure(figsize=(8, 5))\n",
    "#         plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "#         plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "#         plt.xlabel(\"Epoch\")\n",
    "#         plt.ylabel(\"ELBO Loss\")\n",
    "#         plt.title(f\"Train vs Validation Loss - Outer Fold {fold}\")\n",
    "#         plt.legend()\n",
    "#         plt.grid(True)\n",
    "#         plt.tight_layout()\n",
    "#         fname = f\"M4_{output_prefix}{fold}_loss_curve.png\"\n",
    "#         plt.savefig(fname, dpi=300)\n",
    "#         plt.close()\n",
    "#         print(f\"Saved: {fname}\")\n",
    "        \n",
    "# plot_loss_curves(results_df.to_dict('records'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332cee7e-da54-4138-886e-2534740bd511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
