{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0dea974-236c-4f81-9b2e-9c041f34fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import pyro.distributions as dist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "# For reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "pyro.set_rng_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d9796-8d37-44e3-8b53-1045b6670eca",
   "metadata": {},
   "source": [
    "# Model 5a KMeans on Scaled Features (inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3513c05e-d59d-403d-b137-511cbbbfa4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   B01001_001E  Household Income Distribution  Median Household Income  \\\n",
      "0       1675.0                          860.0                  75444.0   \n",
      "1       2221.0                         1070.0                 140313.0   \n",
      "2       1904.0                         1098.0                  83750.0   \n",
      "3       1147.0                          517.0                  62054.0   \n",
      "4       4946.0                         3231.0                  29737.0   \n",
      "\n",
      "   B19001_002E  B19001_003E  B19001_004E  B19001_005E  B19001_006E  \\\n",
      "0         23.0         98.0         21.0         10.0         52.0   \n",
      "1         45.0         24.0         16.0         17.0          0.0   \n",
      "2         72.0         76.0         50.0         72.0          0.0   \n",
      "3          0.0          0.0          0.0         60.0         61.0   \n",
      "4        373.0        471.0        391.0        176.0        217.0   \n",
      "\n",
      "   B19001_007E  B19001_008E  ...  Longitude  storm_county    ppt_mean  \\\n",
      "0         10.0          9.0  ...   -80.3117  201007_12086  117.744385   \n",
      "1         32.0         16.0  ...   -80.3117  201007_12086  117.744385   \n",
      "2         60.0         38.0  ...   -80.3117  201007_12086  117.744385   \n",
      "3         48.0          0.0  ...   -80.3117  201007_12086  117.744385   \n",
      "4        250.0        176.0  ...   -80.3117  201007_12086  117.744385   \n",
      "\n",
      "   tmean_mean  Year  cluster  pct_unemp  pct_crowd    log_pop  log_medinc  \n",
      "0    23.45584  2010        2   0.042706   0.043913  14.709709   11.231159  \n",
      "1    23.45584  2010        2   0.042706   0.043913  14.709709   11.851638  \n",
      "2    23.45584  2010        2   0.042706   0.043913  14.709709   11.335603  \n",
      "3    23.45584  2010        2   0.042706   0.043913  14.709709   11.035776  \n",
      "4    23.45584  2010        2   0.042706   0.043913  14.709709   10.300181  \n",
      "\n",
      "[5 rows x 60 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_95972/4225642204.py:7: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "file_path = \"cleaned_hurricane_damage_data.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd1e72b-75f6-4f31-ae6a-dddd5660c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 746 | Test size: 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/9thmbny930lcn8vcx6g216br0000gp/T/ipykernel_95972/400946517.py:115: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full = pd.read_csv(\"cleaned_hurricane_damage_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ─────────────────────────── Column Mapping ───────────────────────────\n",
    "census_svi_column_mapping = {\n",
    "    'B01001_001E': 'Total Population',\n",
    "    'B19001_001E': 'Household Income Distribution',\n",
    "    'B19013_001E': 'Median Household Income',\n",
    "    'B19001_002E': 'Income Bracket 1',\n",
    "    'B19001_003E': 'Income Bracket 2',\n",
    "    'B19001_004E': 'Income Bracket 3',\n",
    "    'B19001_005E': 'Income Bracket 4',\n",
    "    'B19001_006E': 'Income Bracket 5',\n",
    "    'B19001_007E': 'Income Bracket 6',\n",
    "    'B19001_008E': 'Income Bracket 7',\n",
    "    'B19001_009E': 'Income Bracket 8',\n",
    "    'B19001_010E': 'Income Bracket 9',\n",
    "    'B19001_011E': 'Income Bracket 10',\n",
    "    'B19001_012E': 'Income Bracket 11',\n",
    "    'B19001_013E': 'Income Bracket 12',\n",
    "    'B19001_014E': 'Income Bracket 13',\n",
    "    'B19001_015E': 'Income Bracket 14',\n",
    "    'B19001_016E': 'Income Bracket 15',\n",
    "    'B19001_017E': 'Income Bracket 16',\n",
    "    'state': 'State',\n",
    "    'county': 'County',\n",
    "    'tract': 'Census Tract',\n",
    "    'year': 'Year',\n",
    "    'STATE_COUNTY_FIPS': 'State-County FIPS Code',\n",
    "    'GEOID': 'Geographic Identifier',\n",
    "    'FIPS': 'FIPS Code',\n",
    "    'RPL_THEMES': 'SVI Themes',\n",
    "    'CZ_FIPS': 'Combined Statistical Area FIPS',\n",
    "    'ST': 'State Abbreviation',\n",
    "    'LOCATION': 'Location',\n",
    "    'E_TOTPOP': 'Estimated Total Population',\n",
    "    'M_TOTPOP': 'Margin of Error Total Population',\n",
    "    'E_HU': 'Estimated Housing Units',\n",
    "    'M_HU': 'Margin of Error Housing Units',\n",
    "    'E_UNEMP': 'Estimated Unemployed',\n",
    "    'M_UNEMP': 'Margin of Error Unemployed',\n",
    "    'E_LIMENG': 'Estimated Limited English Proficiency',\n",
    "    'M_LIMENG': 'Margin of Error Limited English Proficiency',\n",
    "    'E_MUNIT': 'Estimated Multi-Unit Housing',\n",
    "    'M_MUNIT': 'Margin of Error Multi-Unit Housing',\n",
    "    'E_MOBILE': 'Estimated Mobile Homes',\n",
    "    'M_MOBILE': 'Margin of Error Mobile Homes',\n",
    "    'E_CROWD': 'Estimated Crowded Housing',\n",
    "    'M_CROWD': 'Margin of Error Crowded Housing',\n",
    "    'E_NOVEH': 'Estimated No Vehicle',\n",
    "    'M_NOVEH': 'Margin of Error No Vehicle',\n",
    "    'DAMAGE_PROPERTY': 'Damage to Property',\n",
    "    'DAMAGE_CROPS': 'Damage to Crops',\n",
    "    'BEGIN_YEARMONTH': 'Begin Year-Month',\n",
    "    'LAT': 'Latitude',\n",
    "    'LON': 'Longitude',\n",
    "    'storm_county': 'Storm County'\n",
    "}\n",
    "\n",
    "\n",
    "# ─────────────────────────── Feature Engineering ───────────────────────────\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    eps = 1e-6\n",
    "\n",
    "    df['pct_unemp']  = df['Estimated Unemployed'] / (df['Total Population'] + eps)\n",
    "    df['pct_limm']   = df['Estimated Limited English Proficiency'] / (df['Total Population'] + eps)\n",
    "    df['pct_noveh']  = df['Estimated No Vehicle'] / (df['Total Population'] + eps)\n",
    "    df['pct_mobile'] = df['Estimated Mobile Homes'] / (df['Estimated Housing Units'] + eps)\n",
    "    df['pct_crowd']  = df['Estimated Crowded Housing'] / (df['Estimated Housing Units'] + eps)\n",
    "\n",
    "    lows  = ['Income Bracket 1', 'Income Bracket 2', 'Income Bracket 3']\n",
    "    highs = ['Income Bracket 14', 'Income Bracket 15', 'Income Bracket 16']\n",
    "    df['low_inc_pct']  = df[lows].sum(axis=1) / (df['Household Income Distribution'] + eps)\n",
    "    df['high_inc_pct'] = df[highs].sum(axis=1) / (df['Household Income Distribution'] + eps)\n",
    "    df['inc_ineq']     = df['high_inc_pct'] / (df['low_inc_pct'] + eps)\n",
    "\n",
    "    df['log_pop']    = np.log1p(df['Total Population'])\n",
    "    df['log_medinc'] = np.log1p(df['Median Household Income'])\n",
    "    df['pop_poverty'] = df['Total Population'] * df['low_inc_pct']\n",
    "\n",
    "    df['Begin Year-Month'] = pd.to_datetime(df['Begin Year-Month'], format='%Y%m')\n",
    "    df['month'] = df['Begin Year-Month'].dt.month\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define your feature columns\n",
    "features = [\n",
    "    'log_pop', 'log_medinc',\n",
    "    'pct_unemp', 'pct_limm', 'pct_noveh',\n",
    "    'low_inc_pct', 'pop_poverty', 'high_inc_pct',\n",
    "    'ppt_mean', 'tmean_mean'\n",
    "]\n",
    "\n",
    "# ─────────────────────────── Main Pipeline ───────────────────────────\n",
    "# Load and rename\n",
    "full = pd.read_csv(\"cleaned_hurricane_damage_data.csv\")\n",
    "full = full.rename(columns=census_svi_column_mapping)\n",
    "full = full.loc[:, ~full.columns.duplicated()]\n",
    "\n",
    "# Remove rows with no damage\n",
    "full = full[full['Damage to Property'] != 0].dropna(subset=['Damage to Property'])\n",
    "\n",
    "# Add derived features\n",
    "full = add_features(full)\n",
    "\n",
    "# Group target (sum) and features (mean)\n",
    "y_group = full.groupby(['Year', 'State-County FIPS Code'])[['Damage to Property']].sum().reset_index()\n",
    "x_group = full.groupby(['Year', 'State-County FIPS Code'])[features + ['Latitude', 'Longitude']].mean().reset_index()\n",
    "\n",
    "# Merge target and predictors\n",
    "full_data = y_group.merge(x_group, on=['Year', 'State-County FIPS Code'])\n",
    "\n",
    "# ─────────────────────────── Train-Test Split ───────────────────────────\n",
    "train_full = full_data[full_data['Year'] < 2020].reset_index(drop=True)\n",
    "test_full  = full_data[full_data['Year'] == 2020].reset_index(drop=True)\n",
    "\n",
    "# ─────────────────────────── Spatial Clustering ───────────────────────────\n",
    "coords = train_full[['Latitude', 'Longitude']].values\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(coords)\n",
    "train_full['spatial_cluster'] = kmeans.labels_\n",
    "\n",
    "print(f\"Train size: {train_full.shape[0]} | Test size: {test_full.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc801b2a-56c5-4658-9f61-ba4ce147be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_Xy(df):\n",
    "    df = df.dropna(subset=features + ['Damage to Property'])\n",
    "    X = df[features].values\n",
    "    y = np.log1p(df['Damage to Property'].values)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10152628-1c64-4d7c-a2cf-bb2a426403e8",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7af2f905-586d-44e5-9603-3000d10bdbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import after kernel reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Utility: Compute MAE and RMSE\n",
    "def safe_metrics(y_true, y_pred):\n",
    "    mask = (~np.isnan(y_true)) & (~np.isnan(y_pred)) & (~np.isinf(y_true)) & (~np.isinf(y_pred))\n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "    return mae, rmse\n",
    "\n",
    "# Core Function: Nested Spatial CV with KMeans on scaled features for inner clustering\n",
    "def nested_spatial_cv_scaled_kmeans(train_data, features, outer_k=8, inner_k=4, n_epochs=2000):\n",
    "    outer_coords = train_data[['Latitude', 'Longitude']].values\n",
    "    outer_kmeans = KMeans(n_clusters=outer_k, random_state=0).fit(outer_coords)\n",
    "    train_data['outer_cluster'] = outer_kmeans.labels_\n",
    "\n",
    "    outer_results = []\n",
    "    all_error_dfs = []\n",
    "\n",
    "    for outer_fold in range(outer_k):\n",
    "        print(f\"\\n=== OUTER Fold {outer_fold+1}/{outer_k} ===\")\n",
    "        outer_val = train_data[train_data['outer_cluster'] == outer_fold]\n",
    "        outer_train = train_data[train_data['outer_cluster'] != outer_fold].copy()\n",
    "\n",
    "        # Use scaled feature-based KMeans for inner clustering\n",
    "        scaler_inner = StandardScaler()\n",
    "        X_inner = scaler_inner.fit_transform(outer_train[features].fillna(0))\n",
    "        inner_kmeans = KMeans(n_clusters=inner_k, random_state=42).fit(X_inner)\n",
    "        outer_train['inner_cluster'] = inner_kmeans.labels_\n",
    "\n",
    "        best_model_params = None\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for inner_fold in range(inner_k):\n",
    "            print(f\"\\n--- INNER Fold {inner_fold+1}/{inner_k} ---\")\n",
    "            inner_val = outer_train[outer_train['inner_cluster'] == inner_fold]\n",
    "            inner_train = outer_train[outer_train['inner_cluster'] != inner_fold]\n",
    "\n",
    "            if len(inner_val) == 0 or len(inner_train) == 0:\n",
    "                print(f\"Skipping empty inner fold {inner_fold}\")\n",
    "                continue\n",
    "\n",
    "            X_train = inner_train[features].values\n",
    "            y_train = np.log1p(inner_train['Damage to Property'].values)\n",
    "            X_val = inner_val[features].values\n",
    "            y_val = np.log1p(inner_val['Damage to Property'].values)\n",
    "\n",
    "            scaler = StandardScaler().fit(X_train)\n",
    "            X_train_t = torch.tensor(scaler.transform(X_train), dtype=torch.float32)\n",
    "            X_val_t = torch.tensor(scaler.transform(X_val), dtype=torch.float32)\n",
    "            y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "            y_val_t = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "            def model(X, y=None):\n",
    "                n_features = X.shape[1]\n",
    "                intercept = pyro.sample(\"intercept\", dist.Normal(0., 10.))\n",
    "                weights = pyro.sample(\"weights\", dist.Normal(torch.zeros(n_features), 10 * torch.ones(n_features)).to_event(1))\n",
    "                sigma = pyro.sample(\"sigma\", dist.HalfNormal(10.))\n",
    "                mu = intercept + (X * weights).sum(-1)\n",
    "                with pyro.plate(\"data\", X.shape[0]):\n",
    "                    pyro.sample(\"obs\", dist.Normal(mu, sigma), obs=y)\n",
    "\n",
    "            def guide(X, y=None):\n",
    "                n_features = X.shape[1]\n",
    "                pyro.sample(\"intercept\", dist.Normal(pyro.param(\"intercept_loc\", torch.tensor(0.0)),\n",
    "                                                     pyro.param(\"intercept_scale\", torch.tensor(1.0), constraint=dist.constraints.positive)))\n",
    "                pyro.sample(\"weights\", dist.Normal(pyro.param(\"weights_loc\", torch.zeros(n_features)),\n",
    "                                                   pyro.param(\"weights_scale\", torch.ones(n_features), constraint=dist.constraints.positive)).to_event(1))\n",
    "                pyro.sample(\"sigma\", dist.HalfNormal(pyro.param(\"sigma_loc\", torch.tensor(1.0), constraint=dist.constraints.positive)))\n",
    "\n",
    "            pyro.clear_param_store()\n",
    "            svi = SVI(model, guide, Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n",
    "\n",
    "            for ep in range(n_epochs):\n",
    "                svi.step(X_train_t, y_train_t)\n",
    "\n",
    "            val_loss = svi.evaluate_loss(X_val_t, y_val_t)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_params = {\n",
    "                    'scaler': scaler,\n",
    "                    'weights': pyro.param(\"weights_loc\").detach().numpy(),\n",
    "                    'intercept': pyro.param(\"intercept_loc\").item()\n",
    "                }\n",
    "\n",
    "        if best_model_params is None:\n",
    "            print(f\"Skipping outer fold {outer_fold} due to no valid inner folds.\")\n",
    "            continue\n",
    "\n",
    "        X_outer = outer_val[features].values\n",
    "        y_outer_log = np.log1p(outer_val['Damage to Property'].values)\n",
    "        X_outer_scaled = best_model_params['scaler'].transform(X_outer)\n",
    "        y_pred_log = best_model_params['intercept'] + X_outer_scaled @ best_model_params['weights']\n",
    "        y_pred_orig = np.expm1(y_pred_log)\n",
    "        y_true_orig = np.expm1(y_outer_log)\n",
    "\n",
    "        mae_log, rmse_log = safe_metrics(y_outer_log, y_pred_log)\n",
    "        mae_orig, rmse_orig = safe_metrics(y_true_orig, y_pred_orig)\n",
    "\n",
    "        error_df = pd.DataFrame({\n",
    "            'true_log': y_outer_log,\n",
    "            'pred_log': y_pred_log,\n",
    "            'true_orig': y_true_orig,\n",
    "            'pred_orig': y_pred_orig,\n",
    "            'Latitude': outer_val['Latitude'].values,\n",
    "            'Longitude': outer_val['Longitude'].values,\n",
    "            'outer_fold': outer_fold\n",
    "        })\n",
    "        #error_df['abs_error_log'] = np.abs(error_df['true_log'] - error_df['pred_log'])\n",
    "        #error_df['abs_error_orig'] = np.abs(error_df['true_orig'] - error_df['pred_orig'])\n",
    "\n",
    "        error_df['abs_error_log'] = np.abs(error_df['true_log'] - error_df['pred_log'])\n",
    "        error_df['rmse_error_log'] = (error_df['true_log'] - error_df['pred_log']) ** 2\n",
    "        error_df['abs_error_orig'] = np.abs(error_df['true_orig'] - error_df['pred_orig'])\n",
    "        error_df['rmse_error_orig'] = (error_df['true_orig'] - error_df['pred_orig']) ** 2\n",
    "\n",
    "        outer_results.append({\n",
    "            'fold': outer_fold,\n",
    "            'mae_log': mae_log,\n",
    "            'rmse_log': rmse_log,\n",
    "            'mae_orig': mae_orig,\n",
    "            'rmse_orig': rmse_orig\n",
    "        })\n",
    "\n",
    "        all_error_dfs.append(error_df)\n",
    "\n",
    "    results_df = pd.DataFrame(outer_results)\n",
    "    full_error_df = pd.concat(all_error_dfs, ignore_index=True)\n",
    "    return results_df, full_error_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45dd5cf9-8746-43bf-b02f-e9e2582680b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OUTER Fold 1/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "\n",
      "=== OUTER Fold 2/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "\n",
      "=== OUTER Fold 3/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "\n",
      "=== OUTER Fold 4/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "\n",
      "=== OUTER Fold 5/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "\n",
      "=== OUTER Fold 6/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "\n",
      "=== OUTER Fold 7/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "\n",
      "--- INNER Fold 4/4 ---\n",
      "\n",
      "=== OUTER Fold 8/8 ===\n",
      "\n",
      "--- INNER Fold 1/4 ---\n",
      "\n",
      "--- INNER Fold 2/4 ---\n",
      "\n",
      "--- INNER Fold 3/4 ---\n",
      "\n",
      "--- INNER Fold 4/4 ---\n"
     ]
    }
   ],
   "source": [
    "# Define your feature columns\n",
    "features = [\n",
    "    'log_pop', 'log_medinc',\n",
    "    'pct_unemp', 'pct_limm', 'pct_noveh',\n",
    "    'low_inc_pct', 'pop_poverty', 'high_inc_pct',\n",
    "    'ppt_mean', 'tmean_mean'\n",
    "]\n",
    "\n",
    "# Run M7 model: Multistage Spatial Model with MLP-based inner clustering\n",
    "results_df_M7, full_error_df_M7 = nested_spatial_cv_scaled_kmeans(\n",
    "    train_data=train_full,\n",
    "    features=features,\n",
    "    outer_k=8,\n",
    "    inner_k=4,\n",
    "    n_epochs=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ee2805f-adb7-429a-aa3c-07bdd0a26212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined error map: M7_combined_log_mae.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "def plot_combined_log_mae(error_df, output_file=\"M7_combined_log_mae.png\"):\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.set_extent([error_df['Longitude'].min() - 1, error_df['Longitude'].max() + 1,\n",
    "                   error_df['Latitude'].min() - 1, error_df['Latitude'].max() + 1], crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.STATES.with_scale('50m'), edgecolor='black', linewidth=0.6)\n",
    "\n",
    "    sc = ax.scatter(\n",
    "        error_df['Longitude'], error_df['Latitude'],\n",
    "        c=error_df['abs_error_log'], cmap='viridis', s=30, alpha=0.8,\n",
    "        transform=ccrs.PlateCarree(), vmin=0, vmax=20\n",
    "    )\n",
    "\n",
    "    cbar = plt.colorbar(sc, ax=ax, orientation='vertical', shrink=0.8, pad=0.02)\n",
    "    cbar.set_label(\"Log MAE\", fontsize=12)\n",
    "    cbar.set_ticks([0, 5, 10, 15, 20, 25])\n",
    "\n",
    "    ax.set_title(\"Log MAE Error (All Outer Folds Combined)\", fontsize=14)\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved combined error map: {output_file}\")\n",
    "plot_combined_log_mae(full_error_df_M7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "378173a1-d85e-4e19-9842-7b8a69868382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train loss and keep track of train loss in the nested_spatial_cv_scaled_kmeans\n",
    "# def plot_loss_curves(outer_results, output_prefix=\"fold\"):\n",
    "#     for result in outer_results:\n",
    "#         train_losses = result['train_losses']\n",
    "#         val_losses = result['val_losses']\n",
    "#         fold = result['fold']\n",
    "\n",
    "#         plt.figure(figsize=(8, 5))\n",
    "#         plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "#         plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "#         plt.xlabel(\"Epoch\")\n",
    "#         plt.ylabel(\"ELBO Loss\")\n",
    "#         plt.title(f\"Train vs Validation Loss - Outer Fold {fold}\")\n",
    "#         plt.legend()\n",
    "#         plt.grid(True)\n",
    "#         plt.tight_layout()\n",
    "#         fname = f\"M7_{output_prefix}{fold}_loss_curve.png\"\n",
    "#         plt.savefig(fname, dpi=300)\n",
    "#         plt.close()\n",
    "#         print(f\"Saved: {fname}\")\n",
    "        \n",
    "# plot_loss_curves(results_df_M7.to_dict('records'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332cee7e-da54-4138-886e-2534740bd511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: M7_scatter_logspace.png\n",
      "Saved: M7_scatter_origspace.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_pred_vs_obs(error_df, log=True, filename=None):\n",
    "    if log:\n",
    "        x = error_df['true_log']\n",
    "        y = error_df['pred_log']\n",
    "        scale = 'Log Scale'\n",
    "        label = 'Log Damage'\n",
    "    else:\n",
    "        x = error_df['true_orig']\n",
    "        y = error_df['pred_orig']\n",
    "        scale = 'Original Scale'\n",
    "        label = 'Damage to Property'\n",
    "\n",
    "    # Scatterplot\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.scatterplot(x=x, y=y, s=20, alpha=0.6, edgecolor=None)\n",
    "    \n",
    "    # Line of perfect prediction\n",
    "    min_val = min(x.min(), y.min())\n",
    "    max_val = max(x.max(), y.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=1.5, label=\"1:1 Line\")\n",
    "\n",
    "    # Labels and aesthetics\n",
    "    plt.xlabel(f\"Observed {label}\")\n",
    "    plt.ylabel(f\"Predicted {label}\")\n",
    "    plt.title(f\"Predicted vs. Observed ({scale})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {filename}\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# ──────────────── Call for both log and original scale ────────────────\n",
    "plot_pred_vs_obs(full_error_df_M7, log=True, filename=\"M7_scatter_logspace.png\")\n",
    "plot_pred_vs_obs(full_error_df_M7, log=False, filename=\"M7_scatter_origspace.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e8ccbc-0393-4485-8c75-ed343412ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the csv directory if it doesn't exist\n",
    "os.makedirs('csv', exist_ok=True)\n",
    "\n",
    "# Save the DataFrames\n",
    "results_df_M7.to_csv('csv/results_df_M7.csv', index=True)\n",
    "full_error_df_M7.to_csv('csv/full_error_df_M7.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d1d800-3399-4e10-9795-bbcaea7f7fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
